{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS and Email Spam Classifier using Naive Bayes (only numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes is a very popular and powerful classifier. One of the reasons it is popular is because it is very efficient. The data processing of the classifier is linear to the number of features since it doesn't involve iterations.<br>\n",
    "\n",
    "Naive Bayes operates under the assumption that all features are independent and that the data's distribution is known. The latter is not always true but there are ways to estimate the distribution (with the Maximum Likelihood Estimation or MLE).<br><br>\n",
    "______ ______ ______ ______ ______ ______ ______ ______ ______ ______ ______<br>\n",
    "\n",
    "__All the coding related to feature generation is the same I already made in my previous 'SMS and Email Classifier notebook' where I used Logistic Regression and Gradient Descent. The 'new' material regarding Naive Bayes starts in the section: Naive Bayes Implementation__<br>\n",
    "______ ______ ______ ______ ______ ______ ______ ______ ______ ______ ______<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was obtained from:<br>\n",
    "__Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Irvine, CA: University of California, School of Information and Computer Science.__<br>\n",
    "\n",
    "Abstract: The SMS Spam Collection is a public set of SMS labeled messages that have been collected for mobile phone spam research.<br>\n",
    "\n",
    "Examples:<br>\n",
    "__ham__ What you doing?how are you?<br>\n",
    "__ham__ Ok lar... Joking wif u oni...<br>\n",
    "__ham__ dun say so early hor... U c already then say...<br>\n",
    "__ham__ MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*<br>\n",
    "__ham__ Siva is in hostel aha:-.<br>\n",
    "__ham__ Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.<br>\n",
    "__spam__ FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop<br>\n",
    "__spam__ Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B<br>\n",
    "__spam__ URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "Loading data first into spam/no spam so I can decide later which features (words/characters/punctuation) to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "y_spam=list()\n",
    "y_no_spam=list()\n",
    "sms_spam=list()\n",
    "sms_no_spam=list()\n",
    "\n",
    "#will also change ham/spam labels to -1/1 to make the math faster when measuring accuracy\n",
    "with open('SMSSpamCollection', 'r') as file:\n",
    "    reader = csv.reader(file,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0]=='ham':\n",
    "            y_no_spam.append(-1)\n",
    "            sms_no_spam.append(row[1])\n",
    "        else:\n",
    "            y_spam.append(1)\n",
    "            sms_spam.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(text):\n",
    "    # This function will perform some changes to the string received. It will first separate\n",
    "    #   some characters so that the combination of total words can be reduced significantly.\n",
    "    #   Then it will remove the stopwords that are always present in sms/emails that doesn't\n",
    "    #   give relevant information.\n",
    "    #   List of stopwords was taken from: https://gist.github.com/sebleier/554280 but I removed\n",
    "    #   some words ('no', 'nor' and 'not').\n",
    "    # Input: \n",
    "    # text : string with complete sms or email message\n",
    "    # Output:\n",
    "    # modified_text: string separating characters/words in 'separators' list and removing stop words \n",
    "    separators=['-','$','Â£',':','+','?','!','/','>','<','*','@','http','.com','www','(',')',\"'\",'.']\n",
    "    stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    text=text.lower()\n",
    "    for separator in separators:\n",
    "        text=text.replace(separator, ' ' + separator + ' ')\n",
    "    text=text.replace(\"   \", \" \")\n",
    "    text=text.replace(\"  \", \" \")\n",
    "    modified_text=str()\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            modified_text=modified_text + word + ' '\n",
    "    return modified_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Processing and Selection\n",
    "The process I will follow is really 'easy'.\n",
    " 1. Will get the top 5% of the words/characters with more 'hits' in the spam texts/emails.\n",
    " 2. Will do the same with the no-spam texts.\n",
    " 3. Will put both in a set (to remove duplicates) named 'features'.\n",
    "\n",
    "I decided to get only 5% of the top features (total words/characters were around 5k) initially because it had a great balance of enough data and speed.\n",
    "As you will see, I made different runs with more and less features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will generate 2 'big' string. One with all the spam texts concatenated and the other one with all the no-spam ones.\n",
    "#this texts are the ones that are going to help me determine the features.\n",
    "spam_text=standardize_data( \" \".join(sms_spam) )\n",
    "no_spam_text=standardize_data( \" \".join(sms_no_spam) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_decision(spam_text,no_spam_text,top):\n",
    "    # This function receive 2 texts and will select the top features in each.\n",
    "    # Input:\n",
    "    # spam_text : string\n",
    "    # no_spam_text : string\n",
    "    # top: proportion of top words/characters to include. Example: 0.05 will be the top 5% of the features\n",
    "    # Output:\n",
    "    # features: union of the 2 set of words/characters from the spam and no-spam texts\n",
    "\n",
    "    total = Counter()\n",
    "    spam = Counter()\n",
    "    no_spam = Counter()\n",
    "\n",
    "    for word in spam_text.split():\n",
    "        spam[word] += 1\n",
    "        total[word] += 1\n",
    "    for word in no_spam_text.split():\n",
    "        no_spam[word] += 1\n",
    "        total[word] += 1\n",
    "\n",
    "    spam_most_common=dict(spam.most_common()[0:int(len(spam)*top)])\n",
    "    no_spam_most_common=dict(no_spam.most_common()[0:int(len(no_spam)*top)])\n",
    "\n",
    "    top_spam_features=set()\n",
    "    top_no_spam_features=set()\n",
    "\n",
    "    for k,v in spam_most_common.items():\n",
    "        top_spam_features.add(k)\n",
    "\n",
    "    for k,v in no_spam_most_common.items():\n",
    "        top_no_spam_features.add(k)\n",
    "\n",
    "    return (top_spam_features | top_no_spam_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of items in the set Features:\n",
      "\n",
      "['probably', 'wait', 'pay', 'wanted', 'next', 'told', 'ready', 'need', 'await', 'everything', 'til', 'together']\n",
      "['person', 'delivery', '6', 'wen', 'cash', 'year', 'call', 'chance', 'alright', 'sms', 'haf', 'wat']\n"
     ]
    }
   ],
   "source": [
    "#will run the previous function to select some features\n",
    "features=feature_decision(spam_text,no_spam_text,0.05)\n",
    "print('Example of items in the set Features:\\n')\n",
    "print(list(features)[0:12])\n",
    "print(list(features)[-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates the vectors according to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(text,features):\n",
    "    # Function receives a string and returns the 'vector' with the features count in the text. It will count each\n",
    "    #   feature word in the text\n",
    "    # Input:\n",
    "    # text: string\n",
    "    # features: features set with top words\n",
    "    # Output:\n",
    "    # array: 1 array with n dimensions. Dimensions are equivalent to the size of features set.\n",
    "    \n",
    "    text=standardize_data(text)\n",
    "    dict_features = { i : 0 for i in features }\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            dict_features[i]+=1\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(list(dict_features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The following process takes a little bit of time. In my computer takes around 15 seconds (Mac 2019).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.704\n",
      "\n",
      "x_train size:  (3920, 475) \ty_train size:  (3920,)\n",
      "\n",
      "x_test size:  (1652, 475) \ty_test size:  (1652,)\n"
     ]
    }
   ],
   "source": [
    "def create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features):\n",
    "    # Function randomly creates training and testing datasets.\n",
    "    #   1: it creates the vectors. 2: concatenates the y's values.  3: adds a column with random values\n",
    "    #   4: uses the random values to divide the train from the test arrays.\n",
    "    # Input:\n",
    "    # y_spam: list of 1's\n",
    "    # sms_spam: list with sms/emails spam messages\n",
    "    # y_no_spam: list of -1's\n",
    "    # sms_no_spam: list with sms/emails no spam messages\n",
    "    # features: features set (words/characters we will be looking for)\n",
    "    # Output:\n",
    "    # x_train: columns same size as the size of the features. Rows depend, but approximately 70% of total.\n",
    "    # y_train: 1 column. Same amount of rows than x_train. Values -1 or 1.\n",
    "    # x_test: columns same size as the size of the features. Rows depend, but approximately 70% of total.\n",
    "    # y_test: 1 column. Same amount of rows than x_test. Values -1 or 1.\n",
    "    \n",
    "    #1: creates vector matrix\n",
    "    for i in range (len(y_spam)):\n",
    "        feature_vector=create_vector(sms_spam[i],features)\n",
    "        feature_vector=feature_vector.reshape(1,feature_vector.shape[0])\n",
    "        if i == 0:\n",
    "            X_spam=feature_vector\n",
    "        else:\n",
    "            X_spam=np.concatenate([X_spam, feature_vector],axis=0)\n",
    "\n",
    "    for i in range (len(y_no_spam)):\n",
    "        feature_vector=create_vector(sms_no_spam[i],features)\n",
    "        feature_vector=feature_vector.reshape(1,feature_vector.shape[0])\n",
    "        if i == 0:\n",
    "            X_no_spam=feature_vector\n",
    "        else:\n",
    "            X_no_spam=np.concatenate([X_no_spam, feature_vector],axis=0)\n",
    "\n",
    "    X=np.concatenate([X_spam, X_no_spam],axis=0)\n",
    "\n",
    "    #2: concatenates y's\n",
    "    y=np.concatenate([y_spam, y_no_spam],axis=0)\n",
    "    X_and_y=np.concatenate([X,y.reshape(y.shape[0],1)],axis=1)\n",
    "\n",
    "    #3: random numbers column\n",
    "    random_column = np.array(np.random.rand(len(y)))  #create random column so I can divide train/test sets\n",
    "    random_column = random_column.reshape(random_column.shape[0],1)\n",
    "    X_and_y=np.concatenate([X_and_y,random_column],axis=1)\n",
    "    train=X_and_y[X_and_y[:,-1]<=.7]\n",
    "    test=X_and_y[X_and_y[:,-1]>.7]\n",
    "\n",
    "    print(\"Proportion of data used for training:\",round(train.shape[0]/(train.shape[0]+test.shape[0]),3 ))\n",
    "\n",
    "    #Train features and y\n",
    "    train=train[:,0:train.shape[1]-1]          #eliminate the random column, we don't need it anymore\n",
    "    y_train=train[:,-1]                        #get y from dataset (which is now the lat column)\n",
    "    x_train=train[:,0:train.shape[1]-1]        #eliminating the y column, this is now our feature set\n",
    "\n",
    "    test=test[:,0:test.shape[1]-1]\n",
    "    y_test=test[:,-1]\n",
    "    x_test=test[:,0:test.shape[1]-1]\n",
    "\n",
    "    #following 2 lines were added to make sure values are 0 or 1 for the features\n",
    "    x_train=np.where(x_train != 0, 1, 0)\n",
    "    x_test=np.where(x_test != 0, 1, 0)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "print('\\nx_train size: ',x_train.shape, '\\ty_train size: ',y_train.shape)\n",
    "print('\\nx_test size: ',x_test.shape, '\\ty_test size: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Implementation\n",
    "The following functions will do all the Naive Bayes probability calculations. Since one of the assumptions is that all variables have the same weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam: 13.39 % \n",
      "Probability of NOT spam: 86.61 %\n"
     ]
    }
   ],
   "source": [
    "def PY(y):\n",
    "    # Function calculates the probability of each class (binary). Its basically of calculation  \n",
    "    #   of the proportion of positive and negative values in the y array.\n",
    "    # Input\n",
    "    # y: binary labels. Same amount of rows as X but with only one 'column'\n",
    "    # Output:\n",
    "    #    positive: probability p(y=1)\n",
    "    #    negative: probability p(y=-1)\n",
    "\n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n = len(y)\n",
    "    pos=(((y == 1)).sum())/(n)\n",
    "    return pos,1-pos\n",
    "\n",
    "positive, negative = PY(y_train)\n",
    "print('Probability of spam:', round(100*positive,2),'%','\\nProbability of NOT spam:',round(100*negative,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PXY(x,y):\n",
    "    # Calculates the probability P(X|Y)\n",
    "    # Input:\n",
    "    #   x: matrix of features\n",
    "    #   y: array of labels or classification of those X features\n",
    "    # Output:\n",
    "    #   prob_positive: probability vector of p(x|y=1)\n",
    "    #   prob_negative: probability vector of p(x|y=-1)\n",
    "\n",
    "    #  '+1 smoothing': add positive/negative samples to avoid division by zero\n",
    "    n, d = x.shape\n",
    "    x = np.concatenate([x, np.ones((2,d)), np.zeros((2,d))])\n",
    "    y = np.concatenate([y, [-1,1,-1,1]])\n",
    "    n, d = x.shape\n",
    "    z=np.concatenate((x, y.reshape(n,1)),axis=1)\n",
    "\n",
    "    X_with_Y_pos = z[z[:, d] == 1]\n",
    "    X_with_Y_pos=np.delete(X_with_Y_pos, d, 1)\n",
    "    total_pos=X_with_Y_pos.shape[0]\n",
    "    prob_positive=X_with_Y_pos.sum(axis=0)/total_pos\n",
    "\n",
    "    X_with_Y_neg = z[z[:, d] == -1]\n",
    "    X_with_Y_neg=np.delete(X_with_Y_neg, d, 1)\n",
    "    total_neg=X_with_Y_neg.shape[0]\n",
    "    prob_negative=X_with_Y_neg.sum(axis=0)/total_neg\n",
    "    \n",
    "    return prob_positive,prob_negative\n",
    "\n",
    "prob_positive, prob_negative = PXY(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(prob_positive, prob_negative, X_test, Y_test):\n",
    "    # Creates the table with the log of the distribution (likelihood). It uses log in order to\n",
    "    #   avoid loosing precision since most of the probabilities can be very small.\n",
    "    #Input:\n",
    "    #  prob_positive: conditional probability for the positive class\n",
    "    #  prob_negative: conditional probabilities for the non-positive class\n",
    "    #  X_test : features\n",
    "    #  Y_test : labels (-1 or +1)\n",
    "    #Output:\n",
    "    #    loglikelihood of each point in X_test (n)\n",
    "    #\n",
    "    n, d = X_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        y=Y_test[i]\n",
    "        x=X_test[i,:]\n",
    "        total=0\n",
    "        for j in range(d):\n",
    "            if y==1 and x[j]>0:\n",
    "                total+=np.log(prob_positive[j])\n",
    "            elif y==1 and x[j]==0:\n",
    "                total+=np.log(1-prob_positive[j])\n",
    "            elif y!=1 and x[j]>0:\n",
    "                total+=np.log(prob_negative[j])\n",
    "            else:\n",
    "                total+=np.log(1-prob_negative[j])\n",
    "        loglikelihood[i]=total\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(positive, negative, prob_positive, prob_negative, X_test):\n",
    "    # Creates the prediction using probabilities. If p(y=1) > p(y=-1) then label is 1, else is -1\n",
    "    # Input:\n",
    "    #   positive: probability of being positive\n",
    "    #   negative: probability of being negative\n",
    "    #   prob_positive: conditional probabilities for the positive class\n",
    "    #   prob_negative: conditional probabilities for the negative class\n",
    "    #   X_test : features\n",
    "    #Output:\n",
    "    #    prediction of each X row\n",
    "    \n",
    "    n, d = X_test.shape\n",
    "    logs=(loglikelihood(prob_positive, prob_negative, X_test, np.ones(n))+np.log(positive))>=(loglikelihood(prob_positive, prob_negative, X_test, -np.ones(n))+np.log(negative))\n",
    "    prediction=logs*1\n",
    "    for i in range(n):\n",
    "        if prediction[i]==0:\n",
    "            prediction[i]=-1\n",
    "        i+=1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred(positive, negative, prob_positive, prob_negative, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_test):\n",
    "    # Calculates accuracy of predictions\n",
    "    # Input:\n",
    "    # y_pred: prediction labels\n",
    "    # y_test: real labels in test set\n",
    "    # Output:\n",
    "    # accuracy: measured from 0 to 1 and multiplied by 100 to get a percentage\n",
    "    scores = y_pred\n",
    "    accuracy = np.mean(y_pred==y_test)\n",
    "    \n",
    "    actual_no_pred_no=0\n",
    "    actual_yes_pred_yes=0\n",
    "    actual_no_pred_yes=0\n",
    "    actual_yes_pred_no=0\n",
    "    \n",
    "    for i in range(scores.shape[0]):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            if y_test[i]==-1:\n",
    "                actual_no_pred_no+=1\n",
    "            else:\n",
    "                actual_yes_pred_yes+=1\n",
    "        else:\n",
    "            if y_test[i]==-1:\n",
    "                actual_no_pred_yes+=1\n",
    "            else:\n",
    "                actual_yes_pred_no+=1\n",
    "    print('\\nCONFUSION MATRIX\\n')\n",
    "    print('\\t PREDICTED')\n",
    "    print('ACTUAL\\t NO\\tYES')\n",
    "    print('------------------------------')\n",
    "    print('NO SPAM\\t',actual_no_pred_no,'\\t',actual_no_pred_yes)\n",
    "    print('SPAM\\t',actual_yes_pred_no,'\\t',actual_yes_pred_yes)\n",
    "    print('\\n')\n",
    "    print(\"Error to minimize (Not spam classified as spam):\",round((100*actual_no_pred_yes)/(actual_no_pred_no+actual_yes_pred_yes+actual_no_pred_yes+actual_yes_pred_no),2),'%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1426 \t 3\n",
      "SPAM\t 16 \t 207\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.18 %\n",
      "Features:  475 \t\tAccuracy (%):  98.85\n"
     ]
    }
   ],
   "source": [
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(y_pred,y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy of 98.85% is good, specially the low (0.18%) of no spam messages classified as spam.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Analysis with Different Amount of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.703\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1426 \t 1\n",
      "SPAM\t 23 \t 203\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.06 %\n",
      "Features:  928 \t\tAccuracy (%):  98.55\n"
     ]
    }
   ],
   "source": [
    "#Run with around TWICE as many features\n",
    "features=feature_decision(spam_text,no_spam_text,0.1)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "prob_positive, prob_negative = PXY(x_test,y_test)\n",
    "y_pred=pred(positive, negative, prob_positive, prob_negative, x_test)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(y_pred,y_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.702\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1424 \t 21\n",
      "SPAM\t 46 \t 169\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 1.27 %\n",
      "Features:  41 \t\tAccuracy (%):  95.96\n"
     ]
    }
   ],
   "source": [
    "#Run with around HALF the features as in the first run\n",
    "features=feature_decision(spam_text,no_spam_text,0.005)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "prob_positive, prob_negative = PXY(x_test,y_test)\n",
    "y_pred=pred(positive, negative, prob_positive, prob_negative, x_test)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(y_pred,y_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.705\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1380 \t 43\n",
      "SPAM\t 152 \t 67\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 2.62 %\n",
      "Features:  7 \t\tAccuracy (%):  88.12\n"
     ]
    }
   ],
   "source": [
    "#Run with a MINIMAL amount of features\n",
    "features=feature_decision(spam_text,no_spam_text,0.001)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "prob_positive, prob_negative = PXY(x_test,y_test)\n",
    "y_pred=pred(positive, negative, prob_positive, prob_negative, x_test)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(y_pred,y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The Naive Bayes classifier is considerably faster than the Logistic Regression with Gradient Descent classifier used for this same example.\n",
    "The main reason is that it calculates the probabilities once and then uses the table to estimate if it is positive or negative.\n",
    "\n",
    "We get a really good prediction accuracy and with more features we can get even better results. During the implementation of Naive Bayes we can incorporate considerably more features than other classifiers that iterate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
